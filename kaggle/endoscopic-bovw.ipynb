{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1928154,"sourceType":"datasetVersion","datasetId":1150130}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Base","metadata":{}},{"cell_type":"code","source":"import os\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nimport cv2\n\nimport joblib\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.vq import vq\n\nnp.random.seed(0)  # reproducibility","metadata":{"execution":{"iopub.status.busy":"2024-09-29T08:42:00.541531Z","iopub.execute_input":"2024-09-29T08:42:00.542473Z","iopub.status.idle":"2024-09-29T08:42:03.078439Z","shell.execute_reply.started":"2024-09-29T08:42:00.542424Z","shell.execute_reply":"2024-09-29T08:42:03.077161Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class BagOfVisualWords:\n    def __init__(\n        self,\n        root_dir: str = \"/kaggle/input/the-hyper-kvasir-dataset/labeled_images\",\n        all_descriptors_dir: str = None,\n        codebook_dir: str = None,\n    ):\n        \"\"\"Constructor method\n        \n        :param all_descriptors_dir: str (optional), path to file including all computed descriptors (vectors)\n        :param codebook_dir: str (optional), path to visual vocabulary\n        \n        \"\"\"\n        self.root_dir = root_dir\n        self.df = pd.read_csv(f\"{root_dir}/image-labels.csv\")\n        self.labels = tuple(self.df[\"Finding\"].unique())\n        \n        # n descriptors of m images extracted from given extractor (description algorithm/ feature detection)\n        if all_descriptors_dir is not None:\n            self.all_descriptors = joblib.load(all_descriptors_dir)\n        \n        # codebook (Lookup table)\n        if codebook_dir is not None:\n            self.k, self.codebook = joblib.load(codebook_dir)\n        \n        # In reality in building codebook, choose small sample size idx for efficient \n        self.samples_idx = []  \n\n    def extract_descriptors(self, method: str = 'sift',\n                                sample_size: int = 2000,\n                                grayscale: bool = True,\n                                strongest_percent: float = 1,\n                                **extractor_kwargs\n                           ) -> list:\n        \"\"\"Extract descriptors from sample_size images\n        :param method: str, method to extract feature descriptors e.g. ORB, SIFT, SURF, etc\n        :param sample_size: size of sample. (We likely use a small sample in real-world scenario,\n            where whole dataset is big)\n\n\n        :return: list, n descriptors x sample_size images\n        \n        # TODO: sample for building visual vocabulary must be balance between classes\n        every class include at least one image\n        \"\"\"\n        # ------ extracting algorithms --------\n        self.method = method\n        if method == \"sift\":\n            self.extractor = cv2.SIFT_create(**extractor_kwargs)\n        elif method == \"orb\":\n            self.extractor = cv2.ORB_create(**extractor_kwargs)\n        elif method == \"surf\":\n            self.extractor = cv2.xfeatures2d.SURF_create(**extractor_kwargs)\n        else:\n            raise ValueError(f\"Unsupported feature detection method: {method}\")\n        \n        # ----- extracting process -------\n        self.sample_idx = np.random.randint(0, len(self.df) + 1, sample_size).tolist() #  randomly sample sample_size images\n\n        descriptors_sample_all = (\n            []\n        )  # each image has many descriptors, descriptors_sample_all\n        # is all descriptors of sample_size images\n\n        # loop each image > extract > append\n        for idx in self.sample_idx:\n            img, _ = self._get_item(idx)\n            # convert to grayscale for efficient computing\n            if len(img.shape) == 3 and grayscale:\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n            # descriptors extracting\n            img_keypoints, img_descriptors = self.extractor.detectAndCompute(img, None)\n            if img_descriptors is not None:\n                # filter top_percent strongest keypoint\n                sorted_couple = sorted(zip(img_keypoints, img_descriptors), key=lambda x: x[0].response, reverse=True)\n                img_keypoints, img_descriptors = zip(*sorted_couple) # unzip\n                top = int(len(img_keypoints) * strongest_percent)\n                top_descriptors = img_descriptors[:top]               \n                \n                for descriptor in top_descriptors:\n                    descriptors_sample_all.append(np.array(descriptor))\n\n        # convert to single numpy array\n        descriptors_sample_all = np.stack(descriptors_sample_all)\n\n        return descriptors_sample_all\n\n    def build_codebook(\n        self,\n        all_descriptors: np.array,\n        k: int = 200,\n    ):\n        \"\"\"Building visual vocabulary (visual words)\n        :param all_descriptors: array of descriptors\n        :param k: #cluster (centroids)\n        :param codebook_path: path to saving codebook\n\n        :return: #centroids, codebook\n\n        \"\"\"\n        kmeans = KMeans(n_clusters=k, random_state=123)\n        kmeans.fit(all_descriptors)\n\n        return kmeans.cluster_centers_\n\n    def get_embedding(self, idx: int, normalized: bool = False, tfidf: bool = False):\n        \"\"\"Get embeddings of image[idx] (image > descriptors > project in codebook > frequencies vectors)\n        :param idx: int, image index\n        :param normalized: bool, if True, normalize embedding in scale [0, 1]\n\n        :return: np.array, frequencies vector (can consider as embedding)\n        \"\"\"\n        img_descriptors = self._get_descriptors(idx)\n        img_visual_words, distance = vq(img_descriptors, self.codebook)\n        img_frequency_vector = np.histogram(\n            img_visual_words, bins=self.k, density=normalized\n        )[0]\n\n        if tfidf:\n            self._tf_idf()\n            img_frequency_vector = img_frequency_vector * self.idf\n\n        return img_frequency_vector\n\n    def _tf_idf(self):\n        \"\"\"TODO: Reweight important features in codebook\"\"\"\n        self.idf = 1\n\n        all_embeddings = []\n        for i in range(len(self.df)):\n            embedding = self.get_embedding(i)\n            all_embeddings.append(embedding)\n\n        all_embeddings = np.stack(all_embeddings)\n\n        N = len(self.df)\n        df = np.sum(all_embeddings > 0, axis=0)\n        idf = np.log(N / df)\n\n        return idf\n\n    def _get_descriptors(self, idx, grayscale=True):\n        \"\"\"Extracting descriptors for each image[idx]\n        :param idx: image index\n\n        :return: np.array, descriptors\n        \"\"\"\n        # get image\n        img, _ = self._get_item(idx)\n        # preprocessing: convert to grayscale for efficient computing\n        if len(img.shape) == 3 and grayscale:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n        # descriptors extracting\n        _, img_descriptors = self.extractor.detectAndCompute(img, None)\n\n        return img_descriptors\n\n    def _get_item(self, idx) -> tuple:\n        \"\"\"Return pair (image(arr), label)\n        :param idx: index of data\n\n        :return: tuple, (image: np.array, label)\n        \"\"\"\n        # get path of image\n        GI_dir = {\"Lower GI\": \"lower-gi-tract\", \"Upper GI\": \"upper-gi-tract\"}\n\n        img = self.df[\"Video file\"][idx]\n        gi_tract = GI_dir[self.df[\"Organ\"][idx]]\n        classification = self.df[\"Classification\"][idx]\n        finding = self.df[\"Finding\"][idx]\n        path = f\"\"\"{self.root_dir}/{gi_tract}/{classification}/{finding}/{img}.jpg\"\"\"\n        assert (\n            os.path.exists(path) == True\n        ), f\"{path} does not exist\"  # dir existance checking\n\n        # read image\n        image = np.array(Image.open(path))\n        label = self.labels.index(finding)\n\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-09-29T08:43:31.256586Z","iopub.execute_input":"2024-09-29T08:43:31.257061Z","iopub.status.idle":"2024-09-29T08:43:31.289025Z","shell.execute_reply.started":"2024-09-29T08:43:31.256989Z","shell.execute_reply":"2024-09-29T08:43:31.287741Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = BagOfVisualWords(\n        root_dir=\"/kaggle/input/hyperkvasir/labeled-images\",\n#         codebook_dir=\"/kaggle/input/bag-of-visual-words/bovw_codebook_sift.pkl\",\n    )\n# 1. extracting descriptors\nall_descriptors = model.extract_descriptors(method='sift', sample_size=1, strongest_percent=1)\nall1_descriptors = model.extract_descriptors(method='sift', sample_size=1, strongest_percent=0.7)\n\nall_descriptors.shape, all1_descriptors.shape\n# joblib.dump(all_descriptors, f'{model.method}sample_all_descriptors.pkl', compress=3) # saving all descriptors\n\n# # 2. building visual vocabulary\n# k = 200\n# all_descriptors = joblib.load('all_descriptors_sift.pkl')\n# codebook = model.build_codebook(all_descriptors, k)\n# # joblib.dump((k, codebook), f'bovw_codebook_{model.method}.pkl', compress=3) # saving codebook\n\n# embedding = model.get_embedding(0, normalized=True)\n# plt.bar(list(range(len(embedding))),embedding)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T08:43:32.628971Z","iopub.execute_input":"2024-09-29T08:43:32.629450Z","iopub.status.idle":"2024-09-29T08:43:33.599570Z","shell.execute_reply.started":"2024-09-29T08:43:32.629406Z","shell.execute_reply":"2024-09-29T08:43:33.598420Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"((1959, 128), (463, 128))"},"metadata":{}}]},{"cell_type":"code","source":"# ------- get strongest_percent keypoints \nimg, label = model._get_item(0)\nextractor = cv2.SIFT_create()\nkeypoints, descriptors = extractor.detectAndCompute(img, None)\n\n# 70% strongest keypoints\npercent = 0.7 \nstrongest = sorted(zip(keypoints, descriptors), key=lambda x: x[0].response, reverse=True)\nkeypoints, descriptors = zip(*strongest) # unzip\ntop = int(len(keypoints) * percent)\ntop_descriptors = descriptors[:top]","metadata":{"execution":{"iopub.status.busy":"2024-09-29T08:07:57.075626Z","iopub.execute_input":"2024-09-29T08:07:57.076080Z","iopub.status.idle":"2024-09-29T08:07:57.255472Z","shell.execute_reply.started":"2024-09-29T08:07:57.076038Z","shell.execute_reply":"2024-09-29T08:07:57.253912Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"791"},"metadata":{}}]},{"cell_type":"code","source":"headers = [f'feature{i}' for i in range(model.k)]\nembedding_df = pd.DataFrame(columns=headers)\nlabels = []\n\n# Embedding entire dataset\nfor idx in range(len(model.df)):\n    img, label = model._get_item(idx)\n    embedding = model.get_embedding(idx, normalized=True)\n    # Add a row to the DataFrame\n    embedding_df.loc[len(embedding_df)] = embedding\n    labels.append(label)\n#     break\n\nembedding_df['label'] = pd.Series(labels, dtype='int')\n\nembedding_df.to_csv('embeddings_with_labels.csv', index=False)\n\n# embedding_df","metadata":{"execution":{"iopub.status.busy":"2024-09-03T16:11:59.805874Z","iopub.execute_input":"2024-09-03T16:11:59.806281Z","iopub.status.idle":"2024-09-03T16:11:59.986952Z","shell.execute_reply.started":"2024-09-03T16:11:59.806243Z","shell.execute_reply":"2024-09-03T16:11:59.985825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classification with embeddings","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# 1. Load dataset\ndf = pd.read_csv('/kaggle/input/bag-of-visual-words/embeddings_with_labels.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-11T04:31:55.148366Z","iopub.execute_input":"2024-09-11T04:31:55.149002Z","iopub.status.idle":"2024-09-11T04:31:56.109387Z","shell.execute_reply.started":"2024-09-11T04:31:55.148954Z","shell.execute_reply":"2024-09-11T04:31:56.108194Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 2. Extract features, labels\nX = df.iloc[:, 0:200]\ny = df.iloc[:, 200]\n\n# Step 3: Divide the dataset into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 4: Train a basic model (Random Forest Classifier in this case)\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# Step 5: Predict on the test set\ny_pred = model.predict(X_test)\n\n# Step 6: Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-09-11T04:32:05.306437Z","iopub.execute_input":"2024-09-11T04:32:05.307086Z","iopub.status.idle":"2024-09-11T04:32:20.301552Z","shell.execute_reply.started":"2024-09-11T04:32:05.307031Z","shell.execute_reply":"2024-09-11T04:32:20.299951Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Accuracy: 58.51%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 7: Perform cross-validation\ncv_scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation\nprint(f'Cross-validation Accuracy: {cv_scores.mean() * 100:.2f}%')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-11T04:32:20.303431Z","iopub.execute_input":"2024-09-11T04:32:20.303851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"\n# =========== Sanity check =================================\n# ====================== Unit tests =================================================\ndef test_attributes():\n    assert model.df.shape == (10662, 4)  # dataframe\n    assert len(model.labels) == 23  # #labels\n\n\n# test _get_item method\ndef test_get_item():\n    image, label = model._get_item(0)\n    assert len(image.shape) == 3  # image is a 3-dimensional array (h, w, c)\n    assert type(label) == int and 0 <= label <= 22  # label\n\n\n# test _get_descriptors method\ndef test_get_descriptors():\n    img_descriptors = model._get_descriptors(0)\n    assert len(img_descriptors.shape) == 2\n\n\n# test extract all descriptors process method\ndef test_extract_desciptors():\n    # all_descriptors = model.extract_descriptors() # ensure output is 2d\n    assert len(model.all_descriptors.shape) == 2, \"Invalid extracting process\"\n    # assert len(model.sample_idx) == 1000, 'Invalid sampling'\n\n\n# test build_codebook method\ndef test_build_codebook():\n    assert model.codebook.shape == (model.k, 128), \"Invalid building codebook process\"\n\n\n# test get_embedding method\ndef test_get_embedding():\n    embedding = model.get_embedding(0)\n    assert embedding.shape[0] == model.k\n    \ntest_attributes()\ntest_get_item()\ntest_get_descriptors()\n# test_extract_desciptors()\ntest_build_codebook()\ntest_get_embedding()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T15:45:33.204945Z","iopub.execute_input":"2024-09-08T15:45:33.205529Z","iopub.status.idle":"2024-09-08T15:45:33.524331Z","shell.execute_reply.started":"2024-09-08T15:45:33.205467Z","shell.execute_reply":"2024-09-08T15:45:33.523152Z"},"trusted":true},"execution_count":4,"outputs":[]}]}