{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Base"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:22:46.983003Z","iopub.status.busy":"2024-09-29T10:22:46.982563Z","iopub.status.idle":"2024-09-29T10:22:49.642771Z","shell.execute_reply":"2024-09-29T10:22:49.641517Z","shell.execute_reply.started":"2024-09-29T10:22:46.982944Z"},"trusted":true},"outputs":[],"source":["import os\n","from collections import Counter\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","import cv2\n","\n","import joblib\n","from sklearn.cluster import KMeans\n","from scipy.cluster.vq import vq\n","\n","np.random.seed(0)  # reproducibility"]},{"cell_type":"markdown","metadata":{},"source":["# BoVW class"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:53:42.629546Z","iopub.status.busy":"2024-09-29T10:53:42.629114Z","iopub.status.idle":"2024-09-29T10:53:42.653049Z","shell.execute_reply":"2024-09-29T10:53:42.651648Z","shell.execute_reply.started":"2024-09-29T10:53:42.629507Z"},"trusted":true},"outputs":[],"source":["from typing import Optional\n","\n","class BagOfVisualWords:\n","    def __init__(\n","        self,\n","        descriptors_lake_path: str = None,    \n","        codebook_dir: str = None,\n","        extractor_kwargs: dict = None,\n","        \n","        root_dir: str = \"/kaggle/input/the-hyper-kvasir-dataset/labeled_images\",\n","        method: str = \"sift\",\n","    ) -> None:\n","        \"\"\"Constructor method\n","        \n","        :param descriptors_lake_path: str (optional), path to file including all computed descriptors (vectors)\n","        :param codebook_dir: str (optional), path to visual vocabulary\n","        \n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.df = pd.read_csv(f\"{root_dir}/image-labels.csv\")\n","        self.labels = tuple(self.df[\"Finding\"].unique())\n","        \n","        # n descriptors of m images extracted from given extractor (description algorithm/ feature detection)\n","        if descriptors_lake_path is not None:\n","            self.descriptors_lake = joblib.load(descriptors_lake_path)\n","        \n","        # codebook (Lookup table)\n","        if codebook_dir is not None:\n","            self.k, self.codebook = joblib.load(codebook_dir)\n","        \n","        # In reality in building codebook, choose small sample size idx for efficient \n","        self.samples_idx = [] \n","        \n","        # ------ extracting algorithms --------\n","        self.method = method\n","        if method == \"sift\":\n","            self.extractor = cv2.SIFT_create(extractor_kwargs)\n","        elif method == \"orb\":\n","            self.extractor = cv2.ORB_create(extractor_kwargs)\n","        elif method == \"surf\":\n","            self.extractor = cv2.xfeatures2d.SURF_create(extractor_kwargs)\n","        else:\n","            raise ValueError(f\"Unsupported feature extracting method: {method}\")\n","\n","    def extract_descriptors(self, sample_size: int = 2000,\n","                                grayscale: bool = True,\n","                                strongest_percent: float = 1,\n","                        ) -> np.array:\n","        \"\"\"Extract descriptors from sample_size images\n","        :param method: str, method to extract feature descriptors e.g. ORB, SIFT, SURF, etc\n","        :param sample_size: size of sample. (We likely use a small sample in real-world scenario,\n","            where whole dataset is big)\n","        :param grayscale: bool - if True, convert to gray for efficient computing\n","        :param strongest_percent: float - get % percent of strongest (based on .response of keypoints)\n","        descriptors.  \n","\n","        :return: list, n descriptors x sample_size images\n","        \n","        # TODO: sample for building visual vocabulary must be balance between classes\n","        every class include at least one image\n","        \"\"\"\n","\n","        # ----- extracting process -------\n","        # == Sampling ==\n","        self.sample_idx = np.random.choice(np.arange(0, len(self.df)),\n","                                            size=sample_size,\n","                                            replace=False\n","                                        ).tolist() #  randomly sample sample_size images\n","\n","        descriptors_sample_all = (\n","            []\n","        )  # each image has many descriptors, descriptors_sample_all\n","        # is a list of all descriptors of sample_size images\n","\n","        # loop each image > extract > append\n","        for idx in self.sample_idx:\n","            img, _ = self._get_item(idx)\n","            # convert to grayscale for efficient computing\n","            if len(img.shape) == 3 and grayscale:\n","                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","            # descriptors extracting\n","            img_keypoints, img_descriptors = self.extractor.detectAndCompute(img, None)\n","            if img_descriptors is not None:\n","                # filter top_percent strongest keypoint\n","                sorted_couple = sorted(zip(img_keypoints, img_descriptors), key=lambda x: x[0].response, reverse=True)\n","                img_keypoints, img_descriptors = zip(*sorted_couple) # unzip\n","                top = int(len(img_keypoints) * strongest_percent)\n","                top_descriptors = img_descriptors[:top]               \n","                \n","                for descriptor in top_descriptors:\n","                    descriptors_sample_all.append(np.array(descriptor))\n","\n","        # convert to single numpy array\n","        descriptors_sample_all = np.stack(descriptors_sample_all)\n","\n","        return descriptors_sample_all\n","\n","    def build_codebook(\n","        self,\n","        descriptors_lake: np.array,\n","        cluster_algorithm: str = 'kmean',\n","        k: int = 200,\n","    ) -> np.array:\n","        \"\"\"Building visual vocabulary (visual words)\n","        :param descriptors_lake: array of descriptors\n","        :param cluster_algorithm: type of cluster algorithm like K-mean, Birch\n","        :param k: #cluster (centroids)\n","        \n","        :return: #centroids, codebook\n","\n","        \"\"\"\n","        kmeans = KMeans(n_clusters=k, random_state=123)\n","        kmeans.fit(descriptors_lake)\n","\n","        return kmeans.cluster_centers_\n","\n","    def get_embedding(self, idx: int, normalized: bool = False, tfidf: bool = False):\n","        \"\"\"Get embeddings of image[idx] (image > descriptors > project in codebook > frequencies vectors)\n","        :param idx: int, image index\n","        :param normalized: bool, if True, normalize embedding in scale [0, 1]\n","\n","        :return: np.array, frequencies vector (can consider as embedding)\n","        \"\"\"\n","        img_descriptors = self._get_descriptors_one_img(idx)\n","        img_visual_words, distance = vq(img_descriptors, self.codebook)\n","        img_frequency_vector = np.histogram(\n","            img_visual_words, bins=self.k, density=normalized\n","        )[0]\n","\n","        if tfidf:\n","            self._tf_idf()\n","            img_frequency_vector = img_frequency_vector * self.idf\n","\n","        return img_frequency_vector\n","\n","    def _tf_idf(self):\n","        \"\"\"TODO: Reweight important features in codebook\"\"\"\n","        self.idf = 1\n","\n","        all_embeddings = []\n","        for i in range(len(self.df)):\n","            embedding = self.get_embedding(i)\n","            all_embeddings.append(embedding)\n","\n","        all_embeddings = np.stack(all_embeddings)\n","\n","        N = len(self.df)\n","        df = np.sum(all_embeddings > 0, axis=0)\n","        idf = np.log(N / df)\n","\n","        return idf\n","\n","    def _get_item(self, idx) -> tuple:\n","        \"\"\"Return pair (image(arr), label)\n","        :param idx: index of data\n","\n","        :return: tuple, (image: np.array, label)\n","        \"\"\"\n","        # get path of image\n","        GI_dir = {\"Lower GI\": \"lower-gi-tract\", \"Upper GI\": \"upper-gi-tract\"}\n","\n","        img = self.df[\"Video file\"][idx]\n","        gi_tract = GI_dir[self.df[\"Organ\"][idx]]\n","        classification = self.df[\"Classification\"][idx]\n","        finding = self.df[\"Finding\"][idx]\n","        path = f\"\"\"{self.root_dir}/{gi_tract}/{classification}/{finding}/{img}.jpg\"\"\"\n","        assert (\n","            os.path.exists(path) == True\n","        ), f\"{path} does not exist\"  # dir existance checking\n","\n","        # read image\n","        image = np.array(Image.open(path))\n","        label = self.labels.index(finding)\n","\n","        return image, label\n","    \n","    def _get_descriptors_one_img(self, idx, grayscale=True):\n","        \"\"\"Extracting descriptors for each image[idx]\n","        :param method: method to extract features e.g. ORB, SIFT, SURF, etc\n","        :param idx: image index\n","\n","        :return: descriptors\n","        :rtype: np.array\n","        \"\"\"\n","        # get image\n","        img, _ = self._get_item(idx)\n","        # preprocessing: convert to grayscale for efficient computing\n","        if len(img.shape) == 3 and grayscale:\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","        # descriptors extracting\n","        _, img_descriptors = self.extractor.detectAndCompute(img, None)\n","\n","        return img_descriptors"]},{"cell_type":"markdown","metadata":{},"source":["# Pipeline"]},{"cell_type":"markdown","metadata":{},"source":["## 1. extracting descriptors"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["'''hyperparameters'''\n","method = 'sift'\n","sample_size = 50 # test\n","# sample_size = len(model.df) # real\n","strongest_percent = 1\n","'''hyperparameters'''\n","\n","# ----- 1. extracting descriptors -------------\n","model = BagOfVisualWords(\n","        root_dir=\"/media/mountHDD2/lamluuduc/endoscopy/dataset/hyperKvasir/labeled-images\",\n","        method=method,\n","    )\n","\n","descriptors_lake = model.extract_descriptors(sample_size=sample_size,\n","                                            strongest_percent=strongest_percent)\n","\n","# == saving checkpoint: all descriptors ==\n","ckpt_path = '../checkpoints'\n","# name convention for saving sample_size-method-strongest.pkl\n","descriptors_lake_path = joblib.dump(descriptors_lake,\n","                                    f'{ckpt_path}/descriptors/{sample_size}_img-{model.method}_extractor-{strongest_percent*100}%_strongest.pkl',\n","                                    compress=3)\n","print('='*20, 'Completely extracting descriptors', '='*20)\n","print(descriptors_lake_path)"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Building codebook"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["descriptors_lake_path = '/media/mountHDD2/lamluuduc/endoscopy/base-code/endoscopic/checkpoints/descriptors/50_img-sift_extractor-100%_strongest.pkl'\n","model = BagOfVisualWords(\n","        root_dir=\"/media/mountHDD2/lamluuduc/endoscopy/dataset/hyperKvasir/labeled-images\",\n","        descriptors_lake_path=descriptors_lake_path\n","    )"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-09-29T10:55:24.734426Z","iopub.status.busy":"2024-09-29T10:55:24.733839Z","iopub.status.idle":"2024-09-29T10:55:29.705982Z","shell.execute_reply":"2024-09-29T10:55:29.704870Z","shell.execute_reply.started":"2024-09-29T10:55:24.734371Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["==================== Completely building codebook ====================\n"]}],"source":["descriptors_lake = model.descriptors_lake\n","\n","# ==============  hyperparameters sweeping =================\n","cluster_algorithm='kmean'\n","k = 200 # #cluster = vector size\n","# ==============  hyperparameters sweeping =================\n","\n","# ------- 2. building visual vocabulary -----------\n","codebook = model.build_codebook(descriptors_lake=descriptors_lake,\n","                                    cluster_algorithm='kmean',\n","                                    k=200\n","                                )\n","print('='*20, 'Completely building codebook', '='*20)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["# ============== unit test building codebook =================\n","assert codebook.shape == (k, 128), \"Invalid building codebook process\"\n","\n","# == saving checkpoint: all descriptors ==\n","ckpt_path = '../checkpoints'\n","# name convention for saving sample_size-method-strongest.pkl\n","descriptors_lake_path = joblib.dump(descriptors_lake,\n","                                    f'{ckpt_path}/codebook/k={k}-{cluster_algorithm}_algorithm.pkl',\n","                                    compress=3)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Embedding\n","- image -> codebook -> embedding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = BagOfVisualWords(\n","        root_dir=\"/media/mountHDD2/lamluuduc/endoscopy/dataset/hyperKvasir/labeled-images\",\n","        codebook_dir=\n","    )"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'BagOfVisualWords' object has no attribute 'codebook'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(embedding))),embedding)\n","Cell \u001b[0;32mIn[42], line 126\u001b[0m, in \u001b[0;36mBagOfVisualWords.get_embedding\u001b[0;34m(self, idx, normalized, tfidf)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get embeddings of image[idx] (image > descriptors > project in codebook > frequencies vectors)\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m:param idx: int, image index\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m:param normalized: bool, if True, normalize embedding in scale [0, 1]\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m:return: np.array, frequencies vector (can consider as embedding)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m img_descriptors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_descriptors_one_img(idx)\n\u001b[0;32m--> 126\u001b[0m img_visual_words, distance \u001b[38;5;241m=\u001b[39m vq(img_descriptors, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodebook\u001b[49m)\n\u001b[1;32m    127\u001b[0m img_frequency_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhistogram(\n\u001b[1;32m    128\u001b[0m     img_visual_words, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, density\u001b[38;5;241m=\u001b[39mnormalized\n\u001b[1;32m    129\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tfidf:\n","\u001b[0;31mAttributeError\u001b[0m: 'BagOfVisualWords' object has no attribute 'codebook'"]}],"source":["embedding = model.get_embedding(0, normalized=True)\n","plt.bar(list(range(len(embedding))),embedding)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-03T16:11:59.806281Z","iopub.status.busy":"2024-09-03T16:11:59.805874Z","iopub.status.idle":"2024-09-03T16:11:59.986952Z","shell.execute_reply":"2024-09-03T16:11:59.985825Z","shell.execute_reply.started":"2024-09-03T16:11:59.806243Z"},"trusted":true},"outputs":[],"source":["headers = [f'feature{i}' for i in range(model.k)]\n","embedding_df = pd.DataFrame(columns=headers)\n","labels = []\n","\n","# Embedding entire dataset\n","for idx in range(len(model.df)):\n","    img, label = model._get_item(idx)\n","    embedding = model.get_embedding(idx, normalized=True)\n","    # Add a row to the DataFrame\n","    embedding_df.loc[len(embedding_df)] = embedding\n","    labels.append(label)\n","#     break\n","\n","embedding_df['label'] = pd.Series(labels, dtype='int')\n","\n","embedding_df.to_csv('embeddings_with_labels.csv', index=False)\n","\n","# embedding_df"]},{"cell_type":"markdown","metadata":{},"source":["# Classification with embeddings"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T04:31:55.149002Z","iopub.status.busy":"2024-09-11T04:31:55.148366Z","iopub.status.idle":"2024-09-11T04:31:56.109387Z","shell.execute_reply":"2024-09-11T04:31:56.108194Z","shell.execute_reply.started":"2024-09-11T04:31:55.148954Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","# 1. Load dataset\n","df = pd.read_csv('/kaggle/input/bag-of-visual-words/embeddings_with_labels.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T04:32:05.307086Z","iopub.status.busy":"2024-09-11T04:32:05.306437Z","iopub.status.idle":"2024-09-11T04:32:20.301552Z","shell.execute_reply":"2024-09-11T04:32:20.299951Z","shell.execute_reply.started":"2024-09-11T04:32:05.307031Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 58.51%\n"]}],"source":["# 2. Extract features, labels\n","X = df.iloc[:, 0:200]\n","y = df.iloc[:, 200]\n","\n","# Step 3: Divide the dataset into training and testing sets (80% training, 20% testing)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Step 4: Train a basic model (Random Forest Classifier in this case)\n","model = RandomForestClassifier(random_state=42)\n","model.fit(X_train, y_train)\n","\n","# Step 5: Predict on the test set\n","y_pred = model.predict(X_test)\n","\n","# Step 6: Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy * 100:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-11T04:32:20.303851Z","iopub.status.busy":"2024-09-11T04:32:20.303431Z"},"trusted":true},"outputs":[],"source":["# Step 7: Perform cross-validation\n","cv_scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation\n","print(f'Cross-validation Accuracy: {cv_scores.mean() * 100:.2f}%')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Test"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-08T15:45:33.205529Z","iopub.status.busy":"2024-09-08T15:45:33.204945Z","iopub.status.idle":"2024-09-08T15:45:33.524331Z","shell.execute_reply":"2024-09-08T15:45:33.523152Z","shell.execute_reply.started":"2024-09-08T15:45:33.205467Z"},"trusted":true},"outputs":[],"source":["\n","# =========== Sanity check =================================\n","# ====================== Unit tests =================================================\n","def test_attributes():\n","    assert model.df.shape == (10662, 4)  # dataframe\n","    assert len(model.labels) == 23  # #labels\n","\n","\n","# test _get_item method\n","def test_get_item():\n","    image, label = model._get_item(0)\n","    assert len(image.shape) == 3  # image is a 3-dimensional array (h, w, c)\n","    assert type(label) == int and 0 <= label <= 22  # label\n","\n","\n","# test _get_descriptors method\n","def test_get_descriptors():\n","    img_descriptors = model._get_descriptors(0)\n","    assert len(img_descriptors.shape) == 2\n","\n","\n","# test extract all descriptors process method\n","def test_extract_descriptors():\n","    # descriptors_lake = model.extract_descriptors() # ensure output is 2d\n","    assert len(model.descriptors_lake.shape) == 2, \"Invalid extracting process\"\n","    # assert len(model.sample_idx) == 1000, 'Invalid sampling'\n","\n","\n","# test build_codebook method\n","def test_build_codebook():\n","    assert model.codebook.shape == (model.k, 128), \"Invalid building codebook process\"\n","\n","\n","# test get_embedding method\n","def test_get_embedding():\n","    embedding = model.get_embedding(0)\n","    assert embedding.shape[0] == model.k\n","    \n","test_attributes()\n","test_get_item()\n","test_get_descriptors()\n","# test_extract_desciptors()\n","test_build_codebook()\n","test_get_embedding()"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1150130,"sourceId":1928154,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
