{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9a1845",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3d540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import vq\n",
    "\n",
    "np.random.seed(0)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "599a139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfVisualWords:\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_data: str = \"/kaggle/input/the-hyper-kvasir-dataset/labeled_images\",\n",
    "        method: str = \"sift\",\n",
    "        all_descriptors_dir: str = None,\n",
    "        codebook_dir: str = None,\n",
    "    ):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        :param root_data: str - path to datatset, defaults to \"/kaggle/input/the-hyper-kvasir-dataset/labeled_images\"\n",
    "        :param method: str (optional) - feature extractor method, defaults to \"sift\"\n",
    "        :param all_descriptors_dir: str (optional), path to file including all computed descriptors (vectors)\n",
    "        :param codebook_dir: str (optional), path to visual vocabulary\n",
    "        \"\"\"\n",
    "        # ---- dataset extracting -----\n",
    "        self.root_data = root_data\n",
    "        self.df = pd.read_csv(f\"{root_data}/image-labels.csv\")\n",
    "        self.labels = tuple(self.df[\"Finding\"].unique())\n",
    "        self.method = method\n",
    "\n",
    "        if method == \"sift\":\n",
    "            self.extractor = cv2.SIFT_create()\n",
    "        elif method == \"orb\":\n",
    "            self.extractor = cv2.ORB_create()\n",
    "        elif method == \"surf\":\n",
    "            self.extractor = cv2.xfeatures2d.SURF_create()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported feature detection method: {method}\")\n",
    "\n",
    "        # n descriptors of m images extracted from given extractor (description algorithm/ feature detection)\n",
    "        if all_descriptors_dir is not None:\n",
    "            self.all_descriptors = joblib.load(all_descriptors_dir)\n",
    "\n",
    "        # codebook (Lookup table)\n",
    "        if codebook_dir is not None:\n",
    "            self.k, self.codebook = joblib.load(codebook_dir)\n",
    "            \n",
    "        self.idf = 1\n",
    "        # In reality in building codebook, choose small sample size idx for efficient \n",
    "        self.samples_idx = [] \n",
    "\n",
    "    def extract_descriptors(self, sample_size=1000,\n",
    "                                grayscale: bool = True,\n",
    "                                strongest_percent: float = 1,\n",
    "                            ):\n",
    "        \"\"\"Extract descriptors from sample_size images\n",
    "        :param method: method to extract feature descriptors e.g. ORB, SIFT, SURF, etc\n",
    "        :param sample_size: size of sample. (We likely use a small sample in real-world scenario,\n",
    "            where whole dataset is big)\n",
    "\n",
    "        :return: all descriptors of sample_size images\n",
    "        :rtype: list\n",
    "\n",
    "        # TODO: sample for building visual vocabulary must be balance between classes\n",
    "        every class include at least one image\n",
    "        \"\"\"\n",
    "        self.sample_idx = np.random.randint(0, len(self.df) + 1, sample_size).tolist()\n",
    "\n",
    "        descriptors_sample_all = (\n",
    "            []\n",
    "        )  # each image has many descriptors, descriptors_sample_all\n",
    "        # is all descriptors of sample_size images\n",
    "\n",
    "        # loop each image > extract > append\n",
    "        for n in self.sample_idx:\n",
    "            # descriptors extracting\n",
    "            img_descriptors = self._get_descriptors(n)\n",
    "            if img_descriptors is not None:\n",
    "                for descriptor in img_descriptors:\n",
    "                    descriptors_sample_all.append(np.array(descriptor))\n",
    "\n",
    "        # convert to single numpy array\n",
    "        descriptors_sample_all = np.stack(descriptors_sample_all)\n",
    "\n",
    "        return descriptors_sample_all\n",
    "\n",
    "    def build_codebook(\n",
    "        self,\n",
    "        all_descriptors: np.array,\n",
    "        k: int = 200,\n",
    "    ):\n",
    "        \"\"\"Building visual vocabulary (visual words)\n",
    "        :param all_descriptors: array of descriptors\n",
    "        :param k: #cluster (centroids)\n",
    "        :param codebook_path: path to saving codebook\n",
    "\n",
    "        :return: #centroids, codebook\n",
    "\n",
    "        \"\"\"\n",
    "        kmeans = KMeans(n_clusters=k, random_state=123)\n",
    "        kmeans.fit(all_descriptors)\n",
    "\n",
    "        return kmeans.cluster_centers_\n",
    "\n",
    "    def get_embedding(self, idx: int, normalized: bool = False, tfidf: bool = False):\n",
    "        \"\"\"Get embeddings of image[idx] (image > descriptors > project in codebook > frequencies vectors)\n",
    "        :param idx: image index\n",
    "\n",
    "        :return: frequencies vector (can consider as embedding)\n",
    "        \"\"\"\n",
    "        img_descriptors = self._get_descriptors(idx)\n",
    "        img_visual_words, distance = vq(img_descriptors, self.codebook)\n",
    "        img_frequency_vector = np.histogram(\n",
    "            img_visual_words, bins=self.k, density=normalized\n",
    "        )[0]\n",
    "\n",
    "        if tfidf:\n",
    "            self._tf_idf()\n",
    "            img_frequency_vector = img_frequency_vector * self.idf\n",
    "\n",
    "        return img_frequency_vector\n",
    "\n",
    "    def _tf_idf(self):\n",
    "        \"\"\"TODO: Reweight important features in codebook\"\"\"\n",
    "\n",
    "        all_embeddings = []\n",
    "        for i in range(len(self.df)):\n",
    "            embedding = self.get_embedding(i)\n",
    "            all_embeddings.append(embedding)\n",
    "\n",
    "        all_embeddings = np.stack(all_embeddings)\n",
    "\n",
    "        N = len(self.df)\n",
    "        df = np.sum(all_embeddings > 0, axis=0)\n",
    "        idf = np.log(N / df)\n",
    "\n",
    "        return idf\n",
    "\n",
    "    def _get_descriptors(self, idx, grayscale=True):\n",
    "        \"\"\"Extracting descriptors for each image[idx]\n",
    "        :param method: method to extract features e.g. ORB, SIFT, SURF, etc\n",
    "        :param idx: image index\n",
    "\n",
    "        :return: descriptors\n",
    "        :rtype: np.array\n",
    "        \"\"\"\n",
    "        # get image\n",
    "        img, _ = self._get_item(idx)\n",
    "        # preprocessing: convert to grayscale for efficient computing\n",
    "        if len(img.shape) == 3 and grayscale:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # descriptors extracting\n",
    "        _, img_descriptors = self.extractor.detectAndCompute(img, None)\n",
    "\n",
    "        return img_descriptors\n",
    "\n",
    "    def _get_item(self, idx):\n",
    "        \"\"\"Return pair (image(arr), label)\n",
    "        :param idx: index of data\n",
    "\n",
    "        :return:\n",
    "            tuple (image: array, label)\n",
    "        \"\"\"\n",
    "        # get path of image\n",
    "        GI_dir = {\"Lower GI\": \"lower-gi-tract\", \"Upper GI\": \"upper-gi-tract\"}\n",
    "\n",
    "        img = self.df[\"Video file\"][idx]\n",
    "        gi_tract = GI_dir[self.df[\"Organ\"][idx]]\n",
    "        classification = self.df[\"Classification\"][idx]\n",
    "        finding = self.df[\"Finding\"][idx]\n",
    "        path = f\"\"\"{self.root_data}/{gi_tract}/{classification}/{finding}/{img}.jpg\"\"\"\n",
    "        assert (\n",
    "            os.path.exists(path) == True\n",
    "        ), \"File does not exist\"  # dir existance checking\n",
    "\n",
    "        # read image\n",
    "        image = np.array(Image.open(path))\n",
    "        label = self.labels.index(finding)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d84ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BagOfVisualWords(\n",
    "        root_dir=\"/media/mountHDD2/lamluuduc/endoscopy/dataset/hyperKvasir/labeled-images\",\n",
    "        codebook_dir=\"/media/mountHDD2/lamluuduc/endoscopy/base-code/endoscopic/checkpoints/bovw_codebook_sift.pkl\",\n",
    "    )\n",
    "# --------- 1. extracting descriptors --------- \n",
    "all_descriptors = model.extract_descriptors(\n",
    "        sample_size=1,\n",
    "        method='sift',\n",
    "    )\n",
    "all_descriptors.shape\n",
    "# joblib.dump(all_descriptors, f'sample_all_descriptors.pkl', compress=3) # saving all descriptors\n",
    "\n",
    "# # 2. building visual vocabulary\n",
    "# k = 200\n",
    "# all_descriptors = joblib.load('all_descriptors_sift.pkl')\n",
    "# codebook = model.build_codebook(all_descriptors, k)\n",
    "# joblib.dump((k, codebook), f'bovw_codebook_{model.method}.pkl', compress=3) # saving codebook\n",
    "\n",
    "# embedding = model.get_embedding(0, normalized=True)\n",
    "# plt.bar(list(range(len(embedding))),embedding)b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b832b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== Sanity check =================================\n",
    "# ====================== Unit tests =================================================\n",
    "# test attributes\n",
    "assert model.df.shape == (10662, 4)  # dataframe\n",
    "assert len(model.labels) == 23\n",
    "\n",
    "# test _get_item method\n",
    "image, label = model._get_item(0)\n",
    "assert len(image.shape) == 3  # image is a 3-dimensional array (h, w, c)\n",
    "assert type(label) == int and 0 <= label <= 23  # label\n",
    "\n",
    "# test _get_descriptors method\n",
    "img_descriptors = model._get_descriptors(0)\n",
    "assert len(img_descriptors.shape) == 2\n",
    "\n",
    "# # test extract all descriptors process method\n",
    "# # ensure output is 2d\n",
    "# assert len(all_descriptors.shape) == 2, 'Invalid extracting process'\n",
    "# assert len(model.sample_idx) == 1000, 'Invalid sampling'\n",
    "\n",
    "# # test build_codebook method\n",
    "# assert model.codebook.shape == (model.k, 128), \"Invalid building codebook process\"\n",
    "\n",
    "# # test get_embedding method\n",
    "# embedding = model.get_embedding(0)\n",
    "# assert embedding.shape[0] == model.k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300ccc1",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e52bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
