{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/teamspace/studios/this_studio/endoscopic'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_extractor': 'SIFT',\n",
       " 'strongest_percent': 1,\n",
       " 'clustering_algorithm': 'KMeans',\n",
       " 'vector_size': 128}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '/teamspace/studios/this_studio/dataset/labeled-images'\n",
    "ckpt_path = './checkpoints'\n",
    "csv_result_path = f'{ckpt_path}/result.csv'\n",
    "\n",
    "os.makedirs(ckpt_path, exist_ok=True)\n",
    "os.makedirs(f'{ckpt_path}/descriptors/', exist_ok=True)\n",
    "os.makedirs(f'{ckpt_path}/codebook/', exist_ok=True)\n",
    "os.makedirs(f'{ckpt_path}/embedding_df/', exist_ok=True)\n",
    "os.makedirs(f'{ckpt_path}/best_model/', exist_ok=True)\n",
    "# os.makedirs(f'{ckpt_path}/{csv_result_path}', exist_ok=True)\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "#  Define search space\n",
    "hyperparam_grid = {\n",
    "    \"feature_extractor\": [\"SIFT\", \"SURF\"],\n",
    "    \"strongest_percent\": [1, 0.7, 0.8],\n",
    "    \"clustering_algorithm\": [\"KMeans\", \"BIRCH\"],\n",
    "    \"vector_size\": [128, 64],\n",
    "}\n",
    "# Generate all combinations of hyperparameters\n",
    "hyperparam_combinations = list(product(*hyperparam_grid.values()))\n",
    "hyperparam_keys = list(hyperparam_grid.keys())\n",
    "\n",
    "\n",
    "# Iterate over all hyperparameter combinations\n",
    "for combination in hyperparam_combinations:\n",
    "    # Create a dictionary of current hyperparameter combination\n",
    "    hyperparams = dict(zip(hyperparam_keys, combination))\n",
    "    break\n",
    "\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:00<00:01, 12.78it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Completely extracting descriptors ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing: 100%|██████████| 10/10 [00:00<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Completely building codebook ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Completely building embedding representation ====================\n",
      "Running TPOT AutoML...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb7eee46cab4dbb9e64dc236063cb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 1.0\n",
      "\n",
      "Generation 2 - Current best internal CV score: 1.0\n",
      "\n",
      "Generation 3 - Current best internal CV score: 1.0\n",
      "\n",
      "Generation 4 - Current best internal CV score: 1.0\n",
      "\n",
      "Generation 5 - Current best internal CV score: 1.0\n",
      "\n",
      "Best pipeline: ExtraTreesClassifier(input_matrix, bootstrap=True, criterion=entropy, max_features=0.1, min_samples_leaf=15, min_samples_split=16, n_estimators=100)\n",
      "Exporting the best model...\n",
      "Best model saved as 'best_model.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "2it [00:00,  9.77it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "3it [00:00,  9.45it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "4it [00:00,  9.25it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "5it [00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics Across All Folds:\n",
      "\n",
      " ================================================== \n",
      "\n",
      "Precision (Macro): 1.0\n",
      "Recall (Macro): 1.0\n",
      "F1-Score (Macro): 1.0\n",
      "\n",
      " ================================================== \n",
      "\n",
      "Precision (Micro): 1.0\n",
      "Recall (Micro): 1.0\n",
      "F1-Score (Micro): 1.0\n",
      "\n",
      " ================================================== \n",
      "\n",
      "Balanced Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 126\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m## ==================== Accumulating saving final result ======================\u001b[39;00m\n\u001b[1;32m    125\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 126\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhyperparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mmetrics\u001b[49m})\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Convert results to a DataFrame and save to CSV\u001b[39;00m\n\u001b[1;32m    128\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "from BoVW import BagOfVisualWords\n",
    "# ===========================================================\n",
    "# Initialization\n",
    "# ===========================================================\n",
    "np.random.seed(0)  # reproducibility\n",
    "root_dir = '/teamspace/studios/this_studio/dataset/labeled-images'\n",
    "ckpt_path = './checkpoints'\n",
    "csv_result_path = f'{ckpt_path}/result.csv'\n",
    "\n",
    "# os.makedirs(ckpt_path, exist_ok=True)\n",
    "# os.makedirs(f'{ckpt_path}/descriptors/', exist_ok=True)\n",
    "# os.makedirs(f'{ckpt_path}/codebook/', exist_ok=True)\n",
    "# os.makedirs(f'{ckpt_path}/embedding_df/', exist_ok=True)\n",
    "# os.makedirs(f'{ckpt_path}/best_model/', exist_ok=True)\n",
    "\n",
    "s_id = np.random.randint(0, 100)\n",
    "\n",
    "# ===========================================================\n",
    "# HYPERPARAMs SWEEPING\n",
    "# ===========================================================\n",
    "'''hyperparameters'''\n",
    "# descriptors extracting\n",
    "method = hyperparams['feature_extractor']\n",
    "strongest_percent = hyperparams['strongest_percent']\n",
    "# quantization\n",
    "cluster_algorithm = hyperparams['clustering_algorithm']\n",
    "k = hyperparams['vector_size'] # #cluster = vector size\n",
    "'''hyperparameters'''\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# ----- 1. extracting descriptors -------------\n",
    "# --------------------------------------------------------------\n",
    "model = BagOfVisualWords(\n",
    "        root_dir=root_dir,\n",
    "        method=method\n",
    "    )\n",
    "# sample_size = len(model.df)\n",
    "sample_size = 20\n",
    "descriptors_lake = model.extract_descriptors(sample_size=sample_size,\n",
    "                                            strongest_percent=strongest_percent)\n",
    "\n",
    "# saving \n",
    "descriptors_lake_path = joblib.dump(descriptors_lake,\n",
    "                                    f'{ckpt_path}/descriptors/id{s_id}-{sample_size}_img-{model.method}_extractor-{strongest_percent*100}%_strongest.pkl',\n",
    "                                    compress=3)\n",
    "del descriptors_lake # free memory\n",
    "print('='*20, 'Completely extracting descriptors', '='*20)\n",
    "\n",
    "# =================== load ================================\n",
    "descriptors_lake = joblib.load(*descriptors_lake_path) # unpack list\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "#  2. building codebook (visual vocabulary) \n",
    "# --------------------------------------------------------------\n",
    "codebook = model.build_codebook(descriptors_lake=descriptors_lake,\n",
    "                                    cluster_algorithm='kmean',\n",
    "                                    k=200\n",
    "                                )\n",
    "# saving \n",
    "codebook_path = joblib.dump(codebook,\n",
    "                            f'{ckpt_path}/codebook/id{s_id}-{cluster_algorithm}_cluster_algorithm-k={k}.pkl',\n",
    "                            compress=3)\n",
    "del descriptors_lake # free memory\n",
    "del codebook # free memory\n",
    "print('='*20, 'Completely building codebook', '='*20)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 3. Embedding representation \n",
    "# --------------------------------------------------------------\n",
    "\n",
    "codebook = joblib.load(*codebook_path) # unpack list\n",
    "# n_imgs = len(model.df)\n",
    "n_imgs = 50 # for test\n",
    "\n",
    "headers = [f'feature{i}' for i in range(codebook.shape[0])]\n",
    "embedding_df = pd.DataFrame(columns=headers)\n",
    "labels = []\n",
    "\n",
    "# Embedding entire dataset\n",
    "normalized = False\n",
    "try:\n",
    "    for idx in tqdm(range(n_imgs)):\n",
    "        img, label = model._get_item(idx)\n",
    "        embedding = model.get_embedding(idx, codebook, normalized=normalized)\n",
    "        # Add a row to the DataFrame\n",
    "        embedding_df.loc[len(embedding_df)] = embedding\n",
    "        labels.append(label)\n",
    "        # break\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "embedding_df['label'] = pd.Series(labels, dtype='int')\n",
    "\n",
    "# saving\n",
    "embedding_df_path = f'{ckpt_path}/embedding_df/id{s_id}-{n_imgs}_img-normalized={normalized}.csv'\n",
    "embedding_df.to_csv(embedding_df_path, index=False)\n",
    "\n",
    "del embedding_df # free memory\n",
    "print('='*20, 'Completely building embedding representation', '='*20)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# 4. Classification \n",
    "# --------------------------------------------------------------\n",
    "df = pd.read_csv(embedding_df_path)\n",
    "\n",
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "\n",
    "from utils import TPOT_autoML, evaluate_model\n",
    "\n",
    "metadata = os.path.basename(embedding_df_path)\n",
    "best_model = TPOT_autoML(X, y, ckpt_path, metadata)\n",
    "# Evaluate the best model using custom metrics\n",
    "metrics = evaluate_model(X, y, best_model)\n",
    "\n",
    "## ==================== Accumulating saving final result ======================\n",
    "results = []\n",
    "results.append({**hyperparams, **metrics})\n",
    "# Convert results to a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# If file exists, append; otherwise, create with headers\n",
    "if csv_result_path:\n",
    "    try:\n",
    "        existing_df = pd.read_csv(csv_result_path)\n",
    "        results_df = pd.concat([existing_df, results_df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "results_df.to_csv(csv_result_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "1it [00:00,  7.91it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "2it [00:00,  8.85it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "3it [00:00,  9.11it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "5it [00:00,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metrics Across All Folds:\n",
      "\n",
      " ================================================== \n",
      "\n",
      "Precision (Macro): 1.0\n",
      "Recall (Macro): 1.0\n",
      "F1-Score (Macro): 1.0\n",
      "\n",
      " ================================================== \n",
      "\n",
      "Precision (Micro): 1.0\n",
      "Recall (Micro): 1.0\n",
      "F1-Score (Micro): 1.0\n",
      "\n",
      " ================================================== \n",
      "\n",
      "Balanced Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(X, y, b_model)\n",
    "\n",
    "## ==================== Accumulating saving final result ======================\n",
    "results = []\n",
    "results.append({**hyperparams, **metrics})\n",
    "# Convert results to a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# If file exists, append; otherwise, create with headers\n",
    "if csv_result_path:\n",
    "    try:\n",
    "        existing_df = pd.read_csv(csv_result_path)\n",
    "        results_df = pd.concat([existing_df, results_df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "results_df.to_csv(csv_result_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TPOT_autoML\n",
    "\n",
    "metadata = os.path.basename(embedding_df_path)\n",
    "b_model = TPOT_autoML(X, y, ckpt_path, metadata)\n",
    "# Evaluate the best model using custom metrics\n",
    "evaluate_model(X, y, b_model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
